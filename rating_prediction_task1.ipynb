{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rating Prediction – Task 1\n",
        "\n",
        "This notebook explores using Google's Gemini models to predict ratings from a Kaggle dataset. We will load and sample the data, design multiple prompt variants, parse structured JSON responses, and build an evaluation harness to compare performance and reliability across prompt versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TASK 1 CONFIGURATION\n",
            "============================================================\n",
            "  EVAL_PROVIDER : openrouter\n",
            "  EVAL_MODEL    : qwen/qwen-2.5-7b-instruct\n",
            "  SAMPLE_SIZE   : 200\n",
            "  EVAL_LIMIT    : Full (all rows)\n",
            "  RANDOM_STATE  : 42\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Edit these values as needed\n",
        "# ============================================================\n",
        "\n",
        "# Evaluation provider: \"openrouter\" (recommended for bulk eval) or \"gemini\"\n",
        "EVAL_PROVIDER = \"openrouter\"\n",
        "\n",
        "# Model to use for evaluation\n",
        "EVAL_MODEL = \"qwen/qwen-2.5-7b-instruct\"\n",
        "\n",
        "# Sample size for stratified sampling\n",
        "SAMPLE_SIZE = 200\n",
        "\n",
        "# Random state for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Evaluation limit: set to None for full eval, or an int (e.g., 20) for quick smoke test\n",
        "EVAL_LIMIT = None  # Set to 20 for quick mode, None for full 200-row evaluation\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 1 CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  EVAL_PROVIDER : {EVAL_PROVIDER}\")\n",
        "print(f\"  EVAL_MODEL    : {EVAL_MODEL}\")\n",
        "print(f\"  SAMPLE_SIZE   : {SAMPLE_SIZE}\")\n",
        "print(f\"  EVAL_LIMIT    : {EVAL_LIMIT or 'Full (all rows)'}\")\n",
        "print(f\"  RANDOM_STATE  : {RANDOM_STATE}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
            "Working directory: /Users/kmanoj/fynd-assessment/task1\n",
            "\n",
            "✓ OPENROUTER_API_KEY found\n",
            "✓ GOOGLE_API_KEY found (for Gemini utilities)\n",
            "\n",
            "Environment validation complete. Ready to proceed.\n"
          ]
        }
      ],
      "source": [
        "# Environment validation\n",
        "import sys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print()\n",
        "\n",
        "# Validate required API keys based on provider\n",
        "if EVAL_PROVIDER == \"openrouter\":\n",
        "    OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "    if not OPENROUTER_API_KEY:\n",
        "        raise ValueError(\n",
        "            \"OPENROUTER_API_KEY is required when EVAL_PROVIDER='openrouter'.\\n\"\n",
        "            \"Set it in your shell: export OPENROUTER_API_KEY=your_key\\n\"\n",
        "            \"Or create a .env file in task1/ with: OPENROUTER_API_KEY=your_key\"\n",
        "        )\n",
        "    print(\"✓ OPENROUTER_API_KEY found\")\n",
        "    \n",
        "elif EVAL_PROVIDER == \"gemini\":\n",
        "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if not GOOGLE_API_KEY:\n",
        "        raise ValueError(\n",
        "            \"GOOGLE_API_KEY is required when EVAL_PROVIDER='gemini'.\\n\"\n",
        "            \"Set it in your shell: export GOOGLE_API_KEY=your_key\\n\"\n",
        "            \"Or create a .env file in task1/ with: GOOGLE_API_KEY=your_key\"\n",
        "        )\n",
        "    print(\"✓ GOOGLE_API_KEY found\")\n",
        "    \n",
        "else:\n",
        "    raise ValueError(f\"Unknown EVAL_PROVIDER: {EVAL_PROVIDER}. Use 'openrouter' or 'gemini'.\")\n",
        "\n",
        "# Also check for Gemini key (needed for repair_to_json and some utilities)\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY:\n",
        "    print(\"✓ GOOGLE_API_KEY found (for Gemini utilities)\")\n",
        "else:\n",
        "    print(\"⚠ GOOGLE_API_KEY not set (Gemini features disabled)\")\n",
        "\n",
        "print()\n",
        "print(\"Environment validation complete. Ready to proceed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Data loading + sampling (~200 rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset shape: (10000, 11)\n",
            "\n",
            "Stars distribution in df_sample:\n",
            "stars\n",
            "1    40\n",
            "2    40\n",
            "3    40\n",
            "4    40\n",
            "5    40\n",
            "Name: count, dtype: int64\n",
            "\n",
            "text_len summary (min/median/max):\n",
            "min= 10 median= 599.0 max= 4960\n",
            "\n",
            "Sample rows (first 3):\n",
            "   stars  text_len                                               text\n",
            "0      3       524  I can understand why everyone loves this place...\n",
            "1      1      1236  I have been to a buffet like this before, but ...\n",
            "2      1       643  I've been boycotting this store since 2009.  O...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Path to the dataset (user can change filename if needed)\n",
        "CSV_PATH = \"../data/yelp.csv\"  # TODO: user can change filename\n",
        "\n",
        "# 1) Load CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# 2) Detect review text and stars columns (case-insensitive search)\n",
        "text_candidates = [\"text\", \"review\", \"review_text\", \"content\", \"comments\", \"Review\", \"Text\"]\n",
        "stars_candidates = [\"stars\", \"rating\", \"score\", \"label\", \"yelp_rating\", \"Stars\", \"Rating\"]\n",
        "\n",
        "lower_to_original = {col.lower(): col for col in df.columns}\n",
        "\n",
        "text_col = None\n",
        "for cand in text_candidates:\n",
        "    col = lower_to_original.get(cand.lower())\n",
        "    if col is not None:\n",
        "        text_col = col\n",
        "        break\n",
        "\n",
        "stars_col = None\n",
        "for cand in stars_candidates:\n",
        "    col = lower_to_original.get(cand.lower())\n",
        "    if col is not None:\n",
        "        stars_col = col\n",
        "        break\n",
        "\n",
        "if text_col is None:\n",
        "    raise ValueError(f\"Could not detect a text column. Checked candidates: {text_candidates}. Available columns: {list(df.columns)}\")\n",
        "\n",
        "if stars_col is None:\n",
        "    raise ValueError(f\"Could not detect a stars/rating column. Checked candidates: {stars_candidates}. Available columns: {list(df.columns)}\")\n",
        "\n",
        "# 3) Rename detected columns to standardized names\n",
        "if text_col != \"text\":\n",
        "    df = df.rename(columns={text_col: \"text\"})\n",
        "if stars_col != \"stars\":\n",
        "    df = df.rename(columns={stars_col: \"stars\"})\n",
        "\n",
        "# 4) Clean data\n",
        "# Drop rows where text is missing or empty after stripping\n",
        "df = df.dropna(subset=[\"text\"])  # ensure we have non-null text\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "\n",
        "df = df[df[\"text\"] != \"\"]\n",
        "\n",
        "# Convert stars to int and keep only values in [1, 2, 3, 4, 5]\n",
        "# First, coerce to numeric\n",
        "stars_numeric = pd.to_numeric(df[\"stars\"], errors=\"coerce\")\n",
        "\n",
        "# Keep only valid integer ratings in the desired range\n",
        "valid_mask = stars_numeric.isin([1, 2, 3, 4, 5])\n",
        "\n",
        "df = df[valid_mask].copy()\n",
        "\n",
        "df[\"stars\"] = stars_numeric[valid_mask].astype(int)\n",
        "\n",
        "# Add text length column\n",
        "df[\"text_len\"] = df[\"text\"].str.len()\n",
        "\n",
        "# 5) Create deterministic sample dataframe\n",
        "# Uses SAMPLE_SIZE and RANDOM_STATE from config cell\n",
        "TARGET_SIZE = SAMPLE_SIZE  # from config\n",
        "\n",
        "if len(df) < TARGET_SIZE:\n",
        "    raise ValueError(f\"Not enough rows after cleaning to create a {TARGET_SIZE}-row sample. Cleaned rows: {len(df)}\")\n",
        "\n",
        "# Prefer stratified sampling by stars (aim for balance across 1-5)\n",
        "per_class_target = TARGET_SIZE // 5  # 40 each if all five star levels are present\n",
        "\n",
        "sample_parts = []\n",
        "used_indices = set()\n",
        "\n",
        "for star_value in range(1, 6):\n",
        "    group = df[df[\"stars\"] == star_value]\n",
        "    if len(group) == 0:\n",
        "        continue\n",
        "    n_take = min(per_class_target, len(group))\n",
        "    if n_take > 0:\n",
        "        part = group.sample(n=n_take, random_state=RANDOM_STATE + star_value)\n",
        "        sample_parts.append(part)\n",
        "        used_indices.update(part.index.tolist())\n",
        "\n",
        "if sample_parts:\n",
        "    df_sample = pd.concat(sample_parts, axis=0)\n",
        "else:\n",
        "    # Fallback: if for some reason no star buckets were sampled, draw purely random sample\n",
        "    df_sample = df.sample(n=TARGET_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "current_size = len(df_sample)\n",
        "\n",
        "if current_size < TARGET_SIZE:\n",
        "    remaining = TARGET_SIZE - current_size\n",
        "    remaining_pool = df.drop(index=df_sample.index, errors=\"ignore\")\n",
        "    if len(remaining_pool) < remaining:\n",
        "        raise ValueError(\n",
        "            f\"Not enough remaining rows to top up sample to {TARGET_SIZE}. \"\n",
        "            f\"Current sample size: {current_size}, remaining pool: {len(remaining_pool)}\"\n",
        "        )\n",
        "    top_up = remaining_pool.sample(n=remaining, random_state=RANDOM_STATE)\n",
        "    df_sample = pd.concat([df_sample, top_up], axis=0)\n",
        "\n",
        "# Ensure deterministic row order (optional but nice for reproducibility)\n",
        "df_sample = df_sample.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "# 6) Print diagnostics\n",
        "print(f\"Cleaned dataset shape: {df.shape}\")\n",
        "print(\"\\nStars distribution in df_sample:\")\n",
        "print(df_sample[\"stars\"].value_counts().sort_index())\n",
        "\n",
        "print(\"\\ntext_len summary (min/median/max):\")\n",
        "text_len_series = df_sample[\"text_len\"]\n",
        "print(\n",
        "    \"min=\", int(text_len_series.min()),\n",
        "    \"median=\", float(text_len_series.median()),\n",
        "    \"max=\", int(text_len_series.max()),\n",
        ")\n",
        "\n",
        "print(\"\\nSample rows (first 3):\")\n",
        "print(df_sample[[\"stars\", \"text_len\", \"text\"]].head(3))\n",
        "\n",
        "# 7) Final assertion\n",
        "assert len(df_sample) == TARGET_SIZE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Gemini client + JSON parsing utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from google import genai\n",
        "\n",
        "# --- A) Environment + Gemini setup ---\n",
        "\n",
        "# Optionally load variables from a .env file in the current directory\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\n",
        "        \"GOOGLE_API_KEY is not set. Set it in your shell (export GOOGLE_API_KEY=...) \"\n",
        "        \"or create a .env file with GOOGLE_API_KEY=YOUR_KEY in the task1 directory.\"\n",
        "    )\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "# --- B) Gemini call wrapper ---\n",
        "\n",
        "def call_gemini(\n",
        "    model_name: str,\n",
        "    prompt: str,\n",
        "    temperature: float = 0.2,\n",
        "    max_output_tokens: int = 512,\n",
        ") -> str:\n",
        "    \"\"\"Call a Gemini model with basic retry + exponential backoff.\n",
        "\n",
        "    Returns the response text.\n",
        "    \"\"\"\n",
        "\n",
        "    generation_config = {\n",
        "        \"temperature\": temperature,\n",
        "        \"max_output_tokens\": max_output_tokens,\n",
        "    }\n",
        "\n",
        "    last_exc: Exception | None = None\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=model_name,\n",
        "                contents=prompt,\n",
        "                config=generation_config,\n",
        "            )\n",
        "\n",
        "            # Prefer response.text when available\n",
        "            text = getattr(response, \"text\", None)\n",
        "\n",
        "            # Fallback: join text parts from candidates/contents\n",
        "            if text is None:\n",
        "                parts: list[str] = []\n",
        "\n",
        "                # Newer google.genai responses expose \"candidates\" with \"content.parts\"\n",
        "                for cand in getattr(response, \"candidates\", []) or []:\n",
        "                    content = getattr(cand, \"content\", None)\n",
        "                    for p in getattr(content, \"parts\", []) or []:\n",
        "                        if getattr(p, \"text\", None):\n",
        "                            parts.append(p.text)\n",
        "\n",
        "                # Also check top-level \"content\" if present\n",
        "                top_content = getattr(response, \"content\", None)\n",
        "                for p in getattr(top_content, \"parts\", []) or []:\n",
        "                    if getattr(p, \"text\", None):\n",
        "                        parts.append(p.text)\n",
        "\n",
        "                if parts:\n",
        "                    text = \"\\n\".join(parts)\n",
        "\n",
        "            if not text:\n",
        "                raise RuntimeError(\"Gemini response did not contain any text content.\")\n",
        "\n",
        "            return text\n",
        "        except Exception as exc:  # noqa: BLE001\n",
        "            last_exc = exc\n",
        "            # Exponential backoff with jitter: 1-1.5s, 2-2.5s, 4-4.5s\n",
        "            sleep_seconds = (2**attempt) + random.uniform(0.0, 0.5)\n",
        "            time.sleep(sleep_seconds)\n",
        "\n",
        "    raise RuntimeError(f\"Gemini call failed after retries: {last_exc}\")\n",
        "\n",
        "\n",
        "# --- C) Robust JSON extraction ---\n",
        "\n",
        "def extract_first_json(raw: str):\n",
        "    \"\"\"Attempt to extract the first JSON object from a raw model string.\n",
        "\n",
        "    Returns (obj, ok, error).\n",
        "    \"\"\"\n",
        "    if raw is None:\n",
        "        return None, False, \"raw response is None\"\n",
        "\n",
        "    if isinstance(raw, str):\n",
        "        text = raw.strip()\n",
        "    else:\n",
        "        return None, False, \"raw response is not a string\"\n",
        "\n",
        "    if not text:\n",
        "        return None, False, \"raw response is empty\"\n",
        "\n",
        "    # Strip code fences ```json ... ``` or ``` ... ```\n",
        "    # Handle cases like ```json\\n{...}\\n``` or ```\\n{...}\\n```.\n",
        "    fence_pattern = r\"^```(?:json)?\\s*(.*?)\\s*```$\"\n",
        "    m = re.match(fence_pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    if m:\n",
        "        text = m.group(1).strip()\n",
        "\n",
        "    # First, try straightforward json.loads\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "        if isinstance(obj, dict):\n",
        "            return obj, True, None\n",
        "        else:\n",
        "            return None, False, \"top-level JSON is not an object (dict)\"\n",
        "    except Exception as e1:  # noqa: BLE001\n",
        "        # Fallback: regex search for first {...} block\n",
        "        try:\n",
        "            brace_pattern = r\"\\{[\\s\\S]*?\\}\"\n",
        "            m2 = re.search(brace_pattern, text, flags=re.DOTALL)\n",
        "            if not m2:\n",
        "                return None, False, f\"could not find JSON object braces: {e1}\"\n",
        "            candidate = m2.group(0)\n",
        "            obj = json.loads(candidate)\n",
        "            if isinstance(obj, dict):\n",
        "                return obj, True, None\n",
        "            else:\n",
        "                return None, False, \"extracted JSON is not an object (dict)\"\n",
        "        except Exception as e2:  # noqa: BLE001\n",
        "            return None, False, f\"failed to parse JSON: primary={e1}, fallback={e2}\"\n",
        "\n",
        "\n",
        "# --- D) Schema validation + prediction function ---\n",
        "\n",
        "def validate_prediction(obj: dict):\n",
        "    \"\"\"Validate prediction schema.\n",
        "\n",
        "    Expected keys:\n",
        "      - predicted_stars: int in [1..5]\n",
        "      - explanation: string (optional, default \"\")\n",
        "\n",
        "    Returns (pred, explanation, ok, error).\n",
        "    \"\"\"\n",
        "    if not isinstance(obj, dict):\n",
        "        return None, None, False, \"prediction object is not a dict\"\n",
        "\n",
        "    # Validate predicted_stars\n",
        "    if \"predicted_stars\" not in obj:\n",
        "        return None, None, False, \"missing 'predicted_stars' key\"\n",
        "\n",
        "    pred_raw = obj[\"predicted_stars\"]\n",
        "    try:\n",
        "        pred_int = int(pred_raw)\n",
        "    except Exception:  # noqa: BLE001\n",
        "        return None, None, False, \"'predicted_stars' is not an int or int-like\"\n",
        "\n",
        "    if pred_int < 1 or pred_int > 5:\n",
        "        return None, None, False, \"'predicted_stars' is outside allowed range [1..5]\"\n",
        "\n",
        "    # Validate explanation\n",
        "    explanation = obj.get(\"explanation\", \"\")\n",
        "    if explanation is None:\n",
        "        explanation = \"\"\n",
        "    if not isinstance(explanation, str):\n",
        "        explanation = str(explanation)\n",
        "\n",
        "    # Truncate explanation to <= 300 chars\n",
        "    if len(explanation) > 300:\n",
        "        explanation = explanation[:300]\n",
        "\n",
        "    return pred_int, explanation, True, None\n",
        "\n",
        "\n",
        "def repair_to_json(model_name: str, raw: str) -> dict | None:\n",
        "    \"\"\"Attempt to repair a malformed or truncated JSON-like model output.\n",
        "\n",
        "    Uses the model itself to complete/fix the JSON into the expected schema.\n",
        "    Returns a result dict shaped like predict_one's output, or None if no repair is attempted.\n",
        "    \"\"\"\n",
        "    if raw is None:\n",
        "        return None\n",
        "\n",
        "    if not isinstance(raw, str):\n",
        "        raw_text = str(raw)\n",
        "    else:\n",
        "        raw_text = raw.strip()\n",
        "\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    repair_prompt = (\n",
        "        \"You are a strict JSON repair assistant.\\n\\n\"\n",
        "        \"Task:\\n\"\n",
        "        \"You are given a raw model output that was supposed to be a JSON object for a rating prediction, \"\n",
        "        \"but it may be truncated, malformed, or contain extra text.\\n\\n\"\n",
        "        \"Your job:\\n\"\n",
        "        \"- Read the raw text.\\n\"\n",
        "        \"- Infer or complete the intended JSON object if possible.\\n\"\n",
        "        \"- Output ONLY a single valid JSON object with this exact schema:\\n\"\n",
        "        \"  {\\\"predicted_stars\\\": <int 1-5>, \\\"explanation\\\": \\\"<brief reasoning>\\\"}\\n\"\n",
        "        \"- Do NOT include markdown, backticks, code fences, or any extra text.\\n\\n\"\n",
        "        \"Raw model output:\\n\"\n",
        "        f\"{raw_text}\\n\"\n",
        "    )\n",
        "\n",
        "    repaired_raw = None\n",
        "    try:\n",
        "        repaired_raw = call_gemini(model_name=model_name, prompt=repair_prompt)\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        return {\n",
        "            \"raw\": repaired_raw,\n",
        "            \"is_valid_json\": 0,\n",
        "            \"schema_ok\": 0,\n",
        "            \"predicted_stars\": None,\n",
        "            \"explanation\": None,\n",
        "            \"error\": f\"repair Gemini call failed: {exc}\",\n",
        "        }\n",
        "\n",
        "    obj, ok_json, err_json = extract_first_json(repaired_raw)\n",
        "    is_valid_json = 1 if ok_json else 0\n",
        "    schema_ok = 0\n",
        "    predicted_stars = None\n",
        "    explanation = None\n",
        "    error_msg = None\n",
        "\n",
        "    if not ok_json:\n",
        "        error_msg = f\"repair JSON parse error: {err_json}\"\n",
        "    else:\n",
        "        pred, expl, ok_schema, err_schema = validate_prediction(obj)\n",
        "        if ok_schema:\n",
        "            schema_ok = 1\n",
        "            predicted_stars = pred\n",
        "            explanation = expl\n",
        "        else:\n",
        "            error_msg = f\"repair schema validation error: {err_schema}\"\n",
        "\n",
        "    return {\n",
        "        \"raw\": repaired_raw,\n",
        "        \"is_valid_json\": is_valid_json,\n",
        "        \"schema_ok\": schema_ok,\n",
        "        \"predicted_stars\": predicted_stars,\n",
        "        \"explanation\": explanation,\n",
        "        \"error\": error_msg,\n",
        "    }\n",
        "\n",
        "\n",
        "def predict_one(model_name: str, prompt_template: str, review_text: str) -> dict:\n",
        "    \"\"\"Run a single prediction given a prompt template and review text.\n",
        "\n",
        "    The template should contain the placeholder {{REVIEW_TEXT}}.\n",
        "    Returns a dict with keys:\n",
        "      ['raw', 'is_valid_json', 'schema_ok', 'predicted_stars', 'explanation', 'error']\n",
        "    \"\"\"\n",
        "    if \"{{REVIEW_TEXT}}\" not in prompt_template:\n",
        "        raise ValueError(\"prompt_template must contain the placeholder {{REVIEW_TEXT}}\");\n",
        "\n",
        "    filled_prompt = prompt_template.replace(\"{{REVIEW_TEXT}}\", review_text)\n",
        "\n",
        "    raw = None\n",
        "    obj = None\n",
        "    is_valid_json = 0\n",
        "    schema_ok = 0\n",
        "    predicted_stars = None\n",
        "    explanation = None\n",
        "    error_msg = None\n",
        "\n",
        "    try:\n",
        "        raw = call_gemini(model_name=model_name, prompt=filled_prompt)\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        error_msg = f\"Gemini call failed: {exc}\"\n",
        "        return {\n",
        "            \"raw\": raw,\n",
        "            \"is_valid_json\": is_valid_json,\n",
        "            \"schema_ok\": schema_ok,\n",
        "            \"predicted_stars\": predicted_stars,\n",
        "            \"explanation\": explanation,\n",
        "            \"error\": error_msg,\n",
        "        }\n",
        "\n",
        "    # First attempt: direct parse + validation\n",
        "    obj, ok_json, err_json = extract_first_json(raw)\n",
        "    if ok_json:\n",
        "        is_valid_json = 1\n",
        "    else:\n",
        "        error_msg = f\"JSON parse error: {err_json}\"\n",
        "\n",
        "    if ok_json:\n",
        "        pred, expl, ok_schema, err_schema = validate_prediction(obj)\n",
        "        if ok_schema:\n",
        "            schema_ok = 1\n",
        "            predicted_stars = pred\n",
        "            explanation = expl\n",
        "        else:\n",
        "            error_msg = f\"Schema validation error: {err_schema}\"\n",
        "\n",
        "    # If initial attempt failed (either JSON or schema), try a single repair pass\n",
        "    if not (is_valid_json and schema_ok):\n",
        "        repaired = repair_to_json(model_name=model_name, raw=raw)\n",
        "        if repaired is not None and repaired.get(\"is_valid_json\") and repaired.get(\"schema_ok\"):\n",
        "            return repaired\n",
        "\n",
        "    return {\n",
        "        \"raw\": raw,\n",
        "        \"is_valid_json\": is_valid_json,\n",
        "        \"schema_ok\": schema_ok,\n",
        "        \"predicted_stars\": predicted_stars,\n",
        "        \"explanation\": explanation,\n",
        "        \"error\": error_msg,\n",
        "    }\n",
        "\n",
        "def predict_one_llm(provider: str, model_name: str, prompt_template: str, review_text: str) -> dict:\n",
        "       \"\"\"Unified prediction entrypoint for Gemini and (optionally) OpenRouter.\n",
        "\n",
        "       provider: \"gemini\" or \"openrouter\".\n",
        "       Returns a dict with keys:\n",
        "         [\"raw\",\"is_valid_json\",\"schema_ok\",\"predicted_stars\",\"explanation\",\"error\",\"provider\",\"model\"]\n",
        "       \"\"\"\n",
        "       provider = provider.lower()\n",
        "       if provider not in {\"gemini\", \"openrouter\"}:\n",
        "           raise ValueError(\"provider must be 'gemini' or 'openrouter'\")\n",
        "\n",
        "       if \"{{REVIEW_TEXT}}\" not in prompt_template:\n",
        "           raise ValueError(\"prompt_template must contain the placeholder {{REVIEW_TEXT}}\")\n",
        "\n",
        "       filled_prompt = prompt_template.replace(\"{{REVIEW_TEXT}}\", review_text)\n",
        "\n",
        "       raw = None\n",
        "       is_valid_json = 0\n",
        "       schema_ok = 0\n",
        "       predicted_stars = None\n",
        "       explanation = None\n",
        "       error_msg = None\n",
        "\n",
        "       # 1) Call the appropriate backend\n",
        "       try:\n",
        "           if provider == \"gemini\":\n",
        "               raw = call_gemini(model_name=model_name, prompt=filled_prompt)\n",
        "           else:\n",
        "               # This assumes you have call_openrouter defined; if not, this branch will error.\n",
        "               raw = call_openrouter(model_name=model_name, prompt=filled_prompt)\n",
        "       except Exception as exc:  # noqa: BLE001\n",
        "           error_msg = f\"{provider} call failed: {exc}\"\n",
        "           return {\n",
        "               \"raw\": raw,\n",
        "               \"is_valid_json\": is_valid_json,\n",
        "               \"schema_ok\": schema_ok,\n",
        "               \"predicted_stars\": predicted_stars,\n",
        "               \"explanation\": explanation,\n",
        "               \"error\": error_msg,\n",
        "               \"provider\": provider,\n",
        "               \"model\": model_name,\n",
        "           }\n",
        "\n",
        "       # 2) Parse + validate JSON\n",
        "       obj, ok_json, err_json = extract_first_json(raw)\n",
        "       if ok_json:\n",
        "           is_valid_json = 1\n",
        "       else:\n",
        "           error_msg = f\"JSON parse error: {err_json}\"\n",
        "\n",
        "       if ok_json:\n",
        "           pred, expl, ok_schema, err_schema = validate_prediction(obj)\n",
        "           if ok_schema:\n",
        "               schema_ok = 1\n",
        "               predicted_stars = pred\n",
        "               explanation = expl\n",
        "           else:\n",
        "               error_msg = f\"Schema validation error: {err_schema}\"\n",
        "\n",
        "       # 3) Optional repair for Gemini only\n",
        "       if provider == \"gemini\" and not (is_valid_json and schema_ok):\n",
        "           repaired = repair_to_json(model_name=model_name, raw=raw)\n",
        "           if repaired is not None and repaired.get(\"is_valid_json\") and repaired.get(\"schema_ok\"):\n",
        "               repaired[\"provider\"] = provider\n",
        "               repaired[\"model\"] = model_name\n",
        "               return repaired\n",
        "\n",
        "       return {\n",
        "           \"raw\": raw,\n",
        "           \"is_valid_json\": is_valid_json,\n",
        "           \"schema_ok\": schema_ok,\n",
        "           \"predicted_stars\": predicted_stars,\n",
        "           \"explanation\": explanation,\n",
        "           \"error\": error_msg,\n",
        "           \"provider\": provider,\n",
        "           \"model\": model_name,\n",
        "       }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'raw': '```json\\n{\\n  \"predicted_stars\": 5,\\n  \"explanation\": \"The review explicitly states \\'Amazing service and great food\\' and a strong intent to return (\\'will come back!\\'), indicating a highly positive experience and complete satisfaction.\"\\n}\\n```', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 5, 'explanation': \"The review explicitly states 'Amazing service and great food' and a strong intent to return ('will come back!'), indicating a highly positive experience and complete satisfaction.\", 'error': None}\n"
          ]
        }
      ],
      "source": [
        "# --- E) Tiny smoke test ---\n",
        "\n",
        "model_name = \"gemini-2.5-flash\"\n",
        "\n",
        "# Very simple prompt template: later cells will harden this to enforce strict JSON.\n",
        "prompt_template = (\n",
        "    \"You are a rating model. Read the following review and respond ONLY with a JSON object \"\n",
        "    \"of the form {\\\"predicted_stars\\\": int, \\\"explanation\\\": string}. The rating must be an integer 1-5.\\n\\n\"\n",
        "    \"Review: {{REVIEW_TEXT}}\"\n",
        ")\n",
        "\n",
        "example_review = \"Amazing service and great food, will come back!\"\n",
        "\n",
        "result = predict_one(model_name=model_name, prompt_template=prompt_template, review_text=example_review)\n",
        "print(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Prompt v1 (baseline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt V1 - Baseline\n",
        "\n",
        "- **Minimal instruction**: Ask the model to act as a strict rating classifier for Yelp reviews.\n",
        "- **Strict JSON schema**: Require a single JSON object with exactly the keys `\"predicted_stars\"` and `\"explanation\"`.\n",
        "- **Brief explanation**: Keep the explanation short (maximum 1-2 sentences of reasoning for the chosen rating).\n",
        "- **No markdown / code fences**: Explicitly forbid markdown, backticks, code fences, or any extra commentary outside the JSON object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_V1 = \"\"\"\n",
        "You are a strict rating classifier.\n",
        "\n",
        "Task:\n",
        "Given a Yelp review, predict the star rating as an integer from 1 to 5.\n",
        "\n",
        "Output rules (critical):\n",
        "- Return ONLY a single valid JSON object.\n",
        "- Do NOT include markdown, code fences, backticks, or any extra text.\n",
        "- Use EXACTLY these keys: \"predicted_stars\" and \"explanation\".\n",
        "- \"predicted_stars\" must be an integer 1, 2, 3, 4, or 5.\n",
        "- \"explanation\" must be brief (max 2 sentences).\n",
        "\n",
        "JSON schema:\n",
        "{\"predicted_stars\": <int 1-5>, \"explanation\": \"<brief reasoning>\"}\n",
        "\n",
        "Review:\n",
        "{{REVIEW_TEXT}}\n",
        "\"\"\".strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Example 0 ---\n",
            "{'raw': '{\"predicted_stars\": 1, \"explanation\": \"The review indicates a significant decline in quality, describing the gelato as grainy with ice crystals and artificial-tasting flavors. This reflects a very poor experience.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 1, 'explanation': 'The review indicates a significant decline in quality, describing the gelato as grainy with ice crystals and artificial-tasting flavors. This reflects a very poor experience.', 'error': None}\n",
            "has_fence: False\n",
            "\n",
            "--- Example 1 ---\n",
            "{'raw': '{\"predicted_stars\": 1, \"explanation\": \"The review is overwhelmingly negative, citing disgusting soup, bland ingredients, potentially spoiled tofu, and fowl-tasting meat, leading the diners to leave without finishing. The strong recommendation against returning justifies the lowest possible rating.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 1, 'explanation': 'The review is overwhelmingly negative, citing disgusting soup, bland ingredients, potentially spoiled tofu, and fowl-tasting meat, leading the diners to leave without finishing. The strong recommendation against returning justifies the lowest possible rating.', 'error': None}\n",
            "has_fence: False\n",
            "\n",
            "--- Example 2 ---\n",
            "{'raw': '{\"predicted_stars\": 1, \"explanation\": \"The store staff called the police on the customer\\'s children for being left alone briefly, despite the children being well-behaved and the customer doing nothing illegal. This extreme overreaction and negative experience warrants the lowest possible rating.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 1, 'explanation': \"The store staff called the police on the customer's children for being left alone briefly, despite the children being well-behaved and the customer doing nothing illegal. This extreme overreaction and negative experience warrants the lowest possible rating.\", 'error': None}\n",
            "has_fence: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test PROMPT_V1 on the first 3 sampled reviews\n",
        "\n",
        "model_name = \"gemini-2.5-flash\"\n",
        "\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading + sampling cell first.\")\n",
        "\n",
        "for i in range(3):\n",
        "    review_text = df_sample.loc[i, \"text\"]\n",
        "    result = predict_one(model_name=model_name, prompt_template=PROMPT_V1, review_text=review_text)\n",
        "    has_fence = \"```\" in (result.get(\"raw\") or \"\")\n",
        "\n",
        "    print(f\"--- Example {i} ---\")\n",
        "    print(result)\n",
        "    print(\"has_fence:\", has_fence)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Robust Example 0 ---\n",
            "is_valid_json: 1 schema_ok: 1\n",
            "{'raw': '{\"predicted_stars\": 2, \"explanation\": \"The review indicates a significant decline in quality.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 2, 'explanation': 'The review indicates a significant decline in quality.', 'error': None}\n",
            "\n",
            "--- Robust Example 1 ---\n",
            "is_valid_json: 0 schema_ok: 0\n",
            "{'raw': None, 'is_valid_json': 0, 'schema_ok': 0, 'predicted_stars': None, 'explanation': None, 'error': \"Gemini call failed: Gemini call failed after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\\\nPlease retry in 12.067704848s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '12s'}]}}\"}\n",
            "\n",
            "--- Robust Example 2 ---\n",
            "is_valid_json: 0 schema_ok: 0\n",
            "{'raw': None, 'is_valid_json': 0, 'schema_ok': 0, 'predicted_stars': None, 'explanation': None, 'error': \"Gemini call failed: Gemini call failed after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\\\nPlease retry in 3.945912914s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}\"}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Robustness test for PROMPT_V1 with repair logic\n",
        "\n",
        "model_name = \"gemini-2.5-flash\"\n",
        "\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading + sampling cell first.\")\n",
        "\n",
        "for i in range(3):\n",
        "    review_text = df_sample.loc[i, \"text\"]\n",
        "    result = predict_one(model_name=model_name, prompt_template=PROMPT_V1, review_text=review_text)\n",
        "\n",
        "    print(f\"--- Robust Example {i} ---\")\n",
        "    print(\"is_valid_json:\", result.get(\"is_valid_json\"), \"schema_ok:\", result.get(\"schema_ok\"))\n",
        "    print(result)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Prompt v2 (rubric + JSON hardening)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt V2 - Rubric + Hard Constraints\n",
        "\n",
        "- Explicit star-rating rubric: Adds a compact 1-5 star rubric to reduce ambiguity in how borderline reviews are scored.\n",
        "- Stricter JSON-only output: Reinforces that the model must return only a single JSON object with the expected keys and no markdown or code fences.\n",
        "- Better mid-range handling: Designed to improve accuracy and consistency on mixed or borderline reviews (especially 2★/3★/4★)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_V2 = \"\"\"\n",
        "You are a strict Yelp star rating classifier.\n",
        "\n",
        "Task:\n",
        "Given a Yelp review, predict the star rating as an integer from 1 to 5.\n",
        "\n",
        "Star rubric (use this to decide):\n",
        "- 1★: Extremely negative. Severe problems, anger, “never again”, “avoid”.\n",
        "- 2★: Mostly negative. Multiple issues outweigh positives; dissatisfaction is clear.\n",
        "- 3★: Mixed or average. Neutral/okay; pros and cons balance; “fine”, “meh”.\n",
        "- 4★: Mostly positive. Minor issues but overall satisfied; would return/recommend.\n",
        "- 5★: Extremely positive. Outstanding experience; strong praise; highly recommend.\n",
        "\n",
        "Decision rule for mixed sentiment:\n",
        "- If clearly leaning positive → 4★\n",
        "- If clearly leaning negative → 2★\n",
        "- Otherwise → 3★\n",
        "\n",
        "Output rules (critical):\n",
        "- Return ONLY a single valid JSON object.\n",
        "- Do NOT include markdown, code fences, backticks, or any extra text.\n",
        "- Use EXACTLY these keys: \"predicted_stars\" and \"explanation\".\n",
        "- \"predicted_stars\" must be an integer 1..5.\n",
        "- \"explanation\" must be brief (max 2 sentences).\n",
        "\n",
        "JSON schema:\n",
        "{\"predicted_stars\": <int 1-5>, \"explanation\": \"<brief reasoning>\"}\n",
        "\n",
        "Review:\n",
        "{{REVIEW_TEXT}}\n",
        "\"\"\".strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 3-star reviews (true label = 3) ===\n",
            "--- idx=92, true_stars=3 ---\n",
            "{'raw': None, 'is_valid_json': 0, 'schema_ok': 0, 'predicted_stars': None, 'explanation': None, 'error': \"Gemini call failed: Gemini call failed after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\\\nPlease retry in 55.785877368s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '55s'}]}}\"}\n",
            "\n",
            "--- idx=78, true_stars=3 ---\n",
            "{'raw': None, 'is_valid_json': 0, 'schema_ok': 0, 'predicted_stars': None, 'explanation': None, 'error': \"Gemini call failed: Gemini call failed after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\\\nPlease retry in 47.458296112s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '47s'}]}}\"}\n",
            "\n",
            "--- idx=75, true_stars=3 ---\n",
            "{'raw': None, 'is_valid_json': 0, 'schema_ok': 0, 'predicted_stars': None, 'explanation': None, 'error': \"Gemini call failed: Gemini call failed after retries: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\\\nPlease retry in 39.300956176s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '39s'}]}}\"}\n",
            "\n",
            "\n",
            "=== 4-star reviews (true label = 4) ===\n",
            "--- idx=99, true_stars=4 ---\n",
            "{'raw': '{\"predicted_stars\": 5, \"explanation\": \"The review uses consistently strong positive language like \\'great\\', \\'wonderful\\', \\'perfect\\', and \\'spot on\\', praising multiple aspects of the store and staff without any negatives. This indicates an outstanding experience.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 5, 'explanation': \"The review uses consistently strong positive language like 'great', 'wonderful', 'perfect', and 'spot on', praising multiple aspects of the store and staff without any negatives. This indicates an outstanding experience.\", 'error': None}\n",
            "\n",
            "--- idx=77, true_stars=4 ---\n",
            "{'raw': '{\"predicted_stars\": 4, \"explanation\": \"The review is overwhelmingly positive, indicating a high level of satisfaction.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 4, 'explanation': 'The review is overwhelmingly positive, indicating a high level of satisfaction.', 'error': None}\n",
            "\n",
            "--- idx=71, true_stars=4 ---\n",
            "{'raw': '{\"predicted_stars\": 4, \"explanation\": \"Despite a significant issue with one dish, the overall experience was still positive.\"}', 'is_valid_json': 1, 'schema_ok': 1, 'predicted_stars': 4, 'explanation': 'Despite a significant issue with one dish, the overall experience was still positive.', 'error': None}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prompt V2 test on ambiguous mid-range reviews\n",
        "\n",
        "model_name = \"gemini-2.5-flash\"\n",
        "\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading + sampling cell first.\")\n",
        "\n",
        "# a) 3 random rows where true stars == 3\n",
        "three_star_rows = df_sample[df_sample[\"stars\"] == 3].sample(n=3, random_state=42)\n",
        "\n",
        "# b) 3 random rows where true stars == 4\n",
        "four_star_rows = df_sample[df_sample[\"stars\"] == 4].sample(n=3, random_state=42)\n",
        "\n",
        "print(\"=== 3-star reviews (true label = 3) ===\")\n",
        "for idx, row in three_star_rows.iterrows():\n",
        "    review_text = row[\"text\"]\n",
        "    true_stars = row[\"stars\"]\n",
        "    result = predict_one(model_name=model_name, prompt_template=PROMPT_V2, review_text=review_text)\n",
        "\n",
        "    print(f\"--- idx={idx}, true_stars={true_stars} ---\")\n",
        "    print(result)\n",
        "    print()\n",
        "\n",
        "print(\"\\n=== 4-star reviews (true label = 4) ===\")\n",
        "for idx, row in four_star_rows.iterrows():\n",
        "    review_text = row[\"text\"]\n",
        "    true_stars = row[\"stars\"]\n",
        "    result = predict_one(model_name=model_name, prompt_template=PROMPT_V2, review_text=review_text)\n",
        "\n",
        "    print(f\"--- idx={idx}, true_stars={true_stars} ---\")\n",
        "    print(result)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "call_openrouter defined: True\n"
          ]
        }
      ],
      "source": [
        "# OpenRouter client setup (standalone cell for reliability)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise ValueError(\"OPENROUTER_API_KEY not found. Add it to task1/.env or export it in your shell.\")\n",
        "\n",
        "def call_openrouter(\n",
        "    model_name: str,\n",
        "    prompt: str,\n",
        "    temperature: float = 0.2,\n",
        "    max_tokens: int = 256,\n",
        "    max_retries: int = 3,\n",
        ") -> str:\n",
        "    \"\"\"Call an OpenRouter chat model with retry + backoff.\"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens,\n",
        "    }\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "            if resp.status_code != 200:\n",
        "                raise RuntimeError(f\"OpenRouter HTTP {resp.status_code}: {resp.text[:500]}\")\n",
        "            data = resp.json()\n",
        "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep((2 ** attempt) + random.uniform(0, 0.5))\n",
        "    raise RuntimeError(f\"OpenRouter call failed after retries: {last_err}\")\n",
        "\n",
        "print(\"call_openrouter defined:\", \"call_openrouter\" in dir())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Prompt V2 on OpenRouter (10-row sanity check) ===\n",
            "\n",
            "idx= 92 | true=3 | pred=3 | schema_ok=1 | expl: Mixed sentiments with more neutral to positive points, but a...\n",
            "idx= 78 | true=3 | pred=3 | schema_ok=1 | expl: The review contains both positive and negative points, with ...\n",
            "idx= 75 | true=3 | pred=2 | schema_ok=1 | expl: The review highlights significant service issues and frustra...\n",
            "idx=144 | true=3 | pred=4 | schema_ok=1 | expl: The review highlights several positive aspects such as the a...\n",
            "idx= 26 | true=3 | pred=3 | schema_ok=1 | expl: The review has both positive and negative points, with no st...\n",
            "idx= 99 | true=4 | pred=4 | schema_ok=1 | expl: The review is predominantly positive with praise for the foo...\n",
            "idx= 77 | true=4 | pred=4 | schema_ok=1 | expl: The review highlights several positive aspects like fresh hu...\n",
            "idx= 71 | true=4 | pred=4 | schema_ok=1 | expl: The review mentions both positive and negative aspects, but ...\n",
            "idx=122 | true=4 | pred=4 | schema_ok=1 | expl: The review mentions several positive aspects such as friendl...\n",
            "idx= 22 | true=4 | pred=4 | schema_ok=1 | expl: The review is mostly positive with satisfaction and praise, ...\n",
            "\n",
            "============================================================\n",
            "\n",
            "schema_ok rate: 100.0% (10/10)\n",
            "exact-match accuracy (where schema_ok=1): 80.0% (8/10)\n"
          ]
        }
      ],
      "source": [
        "# Quick sanity check: Prompt V2 on OpenRouter (10 rows)\n",
        "\n",
        "provider = \"openrouter\"\n",
        "model_name = \"qwen/qwen-2.5-7b-instruct\"\n",
        "\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading + sampling cell first.\")\n",
        "\n",
        "if \"PROMPT_V2\" not in globals():\n",
        "    raise RuntimeError(\"PROMPT_V2 is not defined. Run the Prompt V2 cell first.\")\n",
        "\n",
        "# Sample 5 rows where stars==3, 5 rows where stars==4\n",
        "three_star_sample = df_sample[df_sample[\"stars\"] == 3].sample(n=5, random_state=42)\n",
        "four_star_sample = df_sample[df_sample[\"stars\"] == 4].sample(n=5, random_state=42)\n",
        "\n",
        "test_rows = pd.concat([three_star_sample, four_star_sample])\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"=== Prompt V2 on OpenRouter (10-row sanity check) ===\\n\")\n",
        "\n",
        "for idx, row in test_rows.iterrows():\n",
        "    review_text = row[\"text\"]\n",
        "    true_stars = row[\"stars\"]\n",
        "    \n",
        "    result = predict_one_llm(\n",
        "        provider=provider,\n",
        "        model_name=model_name,\n",
        "        prompt_template=PROMPT_V2,\n",
        "        review_text=review_text,\n",
        "    )\n",
        "    \n",
        "    pred_stars = result.get(\"predicted_stars\")\n",
        "    schema_ok = result.get(\"schema_ok\")\n",
        "    explanation = (result.get(\"explanation\") or \"\")[:60] + \"...\" if result.get(\"explanation\") else \"N/A\"\n",
        "    \n",
        "    results.append({\n",
        "        \"idx\": idx,\n",
        "        \"true_stars\": true_stars,\n",
        "        \"predicted_stars\": pred_stars,\n",
        "        \"schema_ok\": schema_ok,\n",
        "    })\n",
        "    \n",
        "    print(f\"idx={idx:3} | true={true_stars} | pred={pred_stars} | schema_ok={schema_ok} | expl: {explanation}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Summary stats\n",
        "schema_ok_count = sum(1 for r in results if r[\"schema_ok\"] == 1)\n",
        "schema_ok_rate = schema_ok_count / len(results)\n",
        "\n",
        "# Exact match accuracy (only where schema_ok==1)\n",
        "valid_results = [r for r in results if r[\"schema_ok\"] == 1]\n",
        "if valid_results:\n",
        "    exact_matches = sum(1 for r in valid_results if r[\"true_stars\"] == r[\"predicted_stars\"])\n",
        "    accuracy = exact_matches / len(valid_results)\n",
        "else:\n",
        "    accuracy = 0.0\n",
        "\n",
        "print(f\"\\nschema_ok rate: {schema_ok_rate:.1%} ({schema_ok_count}/{len(results)})\")\n",
        "print(f\"exact-match accuracy (where schema_ok=1): {accuracy:.1%} ({exact_matches if valid_results else 0}/{len(valid_results)})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt V3 - Few-shot + Edge Cases\n",
        "\n",
        "- **Few-shot examples**: Adds 5 compact examples to calibrate the model on rubric boundaries and common failure modes.\n",
        "- **Edge case coverage**: Focuses on mixed sentiment (\"good food, bad service\"), short neutral reviews (\"ok\", \"meh\"), and strong praise/complaints.\n",
        "- **Strict JSON output**: Maintains the same JSON-only formatting rules from V2.\n",
        "- **Goal**: Improve accuracy on borderline 2★/3★/4★ reviews where V1/V2 may drift.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_V3 = \"\"\"\n",
        "You are a strict Yelp star rating classifier.\n",
        "\n",
        "Star rubric:\n",
        "- 1★: Extremely negative. Severe problems, anger, \"never again\", \"avoid\".\n",
        "- 2★: Mostly negative. Multiple issues outweigh positives; dissatisfaction is clear.\n",
        "- 3★: Mixed or average. Neutral/okay; pros and cons balance; \"fine\", \"meh\".\n",
        "- 4★: Mostly positive. Minor issues but overall satisfied; would return/recommend.\n",
        "- 5★: Extremely positive. Outstanding experience; strong praise; highly recommend.\n",
        "\n",
        "Decision rule for mixed sentiment:\n",
        "- If clearly leaning positive → 4★\n",
        "- If clearly leaning negative → 2★\n",
        "- Otherwise → 3★\n",
        "\n",
        "Output rules (critical):\n",
        "- Return ONLY a single valid JSON object.\n",
        "- Do NOT include markdown, code fences, backticks, or any extra text.\n",
        "- Use EXACTLY these keys: \"predicted_stars\" and \"explanation\".\n",
        "- \"predicted_stars\" must be an integer 1..5.\n",
        "- \"explanation\" must be brief (max 2 sentences).\n",
        "\n",
        "Few-shot examples:\n",
        "\n",
        "Review: \"Absolutely fantastic. Friendly staff, delicious food, perfect experience.\"\n",
        "Output: {\"predicted_stars\": 5, \"explanation\": \"Strong praise across service and food with no notable negatives.\"}\n",
        "\n",
        "Review: \"It was okay. Nothing special, but not bad either.\"\n",
        "Output: {\"predicted_stars\": 3, \"explanation\": \"Neutral/average sentiment with no strong positives or negatives.\"}\n",
        "\n",
        "Review: \"Food tasted great, but the service was slow and rude. Not sure I'd return.\"\n",
        "Output: {\"predicted_stars\": 3, \"explanation\": \"Clear mix of strong positive (food) and strong negative (service), overall average.\"}\n",
        "\n",
        "Review: \"Great place overall. Loved the ambience and the meal, just a bit pricey.\"\n",
        "Output: {\"predicted_stars\": 4, \"explanation\": \"Mostly positive experience with a minor complaint.\"}\n",
        "\n",
        "Review: \"Terrible experience. Cold food, messy tables, and I will never come back.\"\n",
        "Output: {\"predicted_stars\": 1, \"explanation\": \"Severely negative with multiple major complaints and a strong refusal to return.\"}\n",
        "\n",
        "Now classify this review.\n",
        "\n",
        "Review:\n",
        "{{REVIEW_TEXT}}\n",
        "\"\"\".strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Prompt V3 on OpenRouter (10 x 3★ reviews) ===\n",
            "\n",
            "idx= 79 | true=3 | pred=2 | schema_ok=1 | Multiple issues with service overshadow the positi...\n",
            "idx=195 | true=3 | pred=3 | schema_ok=1 | Mixed sentiment with mostly positive aspects but m...\n",
            "idx=186 | true=3 | pred=3 | schema_ok=1 | Mixed sentiment with a clear issue about the salti...\n",
            "idx= 85 | true=3 | pred=3 | schema_ok=1 | Initial review leaned negative due to pricing issu...\n",
            "idx=171 | true=3 | pred=2 | schema_ok=1 | Multiple issues with dental care and communication...\n",
            "idx=  5 | true=3 | pred=2 | schema_ok=1 | Multiple issues with the dining experience, includ...\n",
            "idx=121 | true=3 | pred=3 | schema_ok=1 | Mixed sentiment with a focus on the unexpected dem...\n",
            "idx= 18 | true=3 | pred=4 | schema_ok=1 | Mostly positive experience with a minor complaint ...\n",
            "idx= 57 | true=3 | pred=4 | schema_ok=1 | Mostly positive experience with a few minor issues...\n",
            "idx=188 | true=3 | pred=3 | schema_ok=1 | Mixed sentiments with significant negatives in atm...\n",
            "\n",
            "============================================================\n",
            "\n",
            "schema_ok rate: 100.0% (10/10)\n",
            "exact-match accuracy on 3★ (where schema_ok=1): 50.0% (5/10)\n"
          ]
        }
      ],
      "source": [
        "# Prompt V3 test on OpenRouter (10 x 3★ reviews)\n",
        "\n",
        "provider = \"openrouter\"\n",
        "model_name = \"qwen/qwen-2.5-7b-instruct\"\n",
        "\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading + sampling cell first.\")\n",
        "\n",
        "if \"PROMPT_V3\" not in globals():\n",
        "    raise RuntimeError(\"PROMPT_V3 is not defined. Run the PROMPT_V3 cell first.\")\n",
        "\n",
        "# 10 random rows where stars == 3 (the hardest class)\n",
        "three_star_sample = df_sample[df_sample[\"stars\"] == 3].sample(n=10, random_state=7)\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"=== Prompt V3 on OpenRouter (10 x 3★ reviews) ===\\n\")\n",
        "\n",
        "for idx, row in three_star_sample.iterrows():\n",
        "    review_text = row[\"text\"]\n",
        "    true_stars = row[\"stars\"]\n",
        "    \n",
        "    result = predict_one_llm(\n",
        "        provider=provider,\n",
        "        model_name=model_name,\n",
        "        prompt_template=PROMPT_V3,\n",
        "        review_text=review_text,\n",
        "    )\n",
        "    \n",
        "    pred_stars = result.get(\"predicted_stars\")\n",
        "    schema_ok = result.get(\"schema_ok\")\n",
        "    explanation = (result.get(\"explanation\") or \"\")[:50] + \"...\" if result.get(\"explanation\") else \"N/A\"\n",
        "    \n",
        "    results.append({\n",
        "        \"idx\": idx,\n",
        "        \"true_stars\": true_stars,\n",
        "        \"predicted_stars\": pred_stars,\n",
        "        \"schema_ok\": schema_ok,\n",
        "    })\n",
        "    \n",
        "    print(f\"idx={idx:3} | true={true_stars} | pred={pred_stars} | schema_ok={schema_ok} | {explanation}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Summary stats\n",
        "schema_ok_count = sum(1 for r in results if r[\"schema_ok\"] == 1)\n",
        "schema_ok_rate = schema_ok_count / len(results)\n",
        "\n",
        "# Exact match accuracy (only where schema_ok==1)\n",
        "valid_results = [r for r in results if r[\"schema_ok\"] == 1]\n",
        "if valid_results:\n",
        "    exact_matches = sum(1 for r in valid_results if r[\"true_stars\"] == r[\"predicted_stars\"])\n",
        "    accuracy = exact_matches / len(valid_results)\n",
        "else:\n",
        "    exact_matches = 0\n",
        "    accuracy = 0.0\n",
        "\n",
        "print(f\"\\nschema_ok rate: {schema_ok_rate:.1%} ({schema_ok_count}/{len(results)})\")\n",
        "print(f\"exact-match accuracy on 3★ (where schema_ok=1): {accuracy:.1%} ({exact_matches}/{len(valid_results)})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔥 FULL MODE: Evaluating all 200 rows\n",
            "\n",
            "============================================================\n",
            "Running V1 on 200 rows (provider=openrouter)...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V1: 100%|██████████| 200/200 [03:38<00:00,  1.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Running V2 on 200 rows (provider=openrouter)...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V2: 100%|██████████| 200/200 [02:51<00:00,  1.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Running V3 on 200 rows (provider=openrouter)...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V3: 100%|██████████| 200/200 [03:38<00:00,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total predictions: 600\n",
            "Provider: openrouter, Model: qwen/qwen-2.5-7b-instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Full Evaluation: PROMPT_V1, PROMPT_V2, PROMPT_V3\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Use config constants\n",
        "provider = EVAL_PROVIDER\n",
        "model_name = EVAL_MODEL\n",
        "\n",
        "# Ensure prerequisites\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading cell first.\")\n",
        "if \"PROMPT_V1\" not in globals() or \"PROMPT_V2\" not in globals() or \"PROMPT_V3\" not in globals():\n",
        "    raise RuntimeError(\"PROMPT_V1/V2/V3 not all defined. Run their definition cells first.\")\n",
        "\n",
        "prompts = {\n",
        "    \"V1\": PROMPT_V1,\n",
        "    \"V2\": PROMPT_V2,\n",
        "    \"V3\": PROMPT_V3,\n",
        "}\n",
        "\n",
        "# Determine evaluation subset\n",
        "if EVAL_LIMIT is not None:\n",
        "    eval_df = df_sample.head(EVAL_LIMIT).copy()\n",
        "    print(f\"🚀 QUICK MODE: Evaluating first {EVAL_LIMIT} rows only\")\n",
        "else:\n",
        "    eval_df = df_sample.copy()\n",
        "    print(f\"🔥 FULL MODE: Evaluating all {len(eval_df)} rows\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for prompt_name, prompt_template in prompts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running {prompt_name} on {len(eval_df)} rows (provider={provider})...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=prompt_name):\n",
        "        review_text = row[\"text\"]\n",
        "        true_stars = row[\"stars\"]\n",
        "        text_len = row[\"text_len\"]\n",
        "        \n",
        "        result = predict_one_llm(\n",
        "            provider=provider,\n",
        "            model_name=model_name,\n",
        "            prompt_template=prompt_template,\n",
        "            review_text=review_text,\n",
        "        )\n",
        "        \n",
        "        all_results.append({\n",
        "            \"prompt\": prompt_name,\n",
        "            \"stars_true\": true_stars,\n",
        "            \"stars_pred\": result.get(\"predicted_stars\"),\n",
        "            \"schema_ok\": result.get(\"schema_ok\"),\n",
        "            \"is_valid_json\": result.get(\"is_valid_json\"),\n",
        "            \"text_len\": text_len,\n",
        "            \"error\": result.get(\"error\"),\n",
        "        })\n",
        "\n",
        "# Create results dataframe\n",
        "df_results = pd.DataFrame(all_results)\n",
        "print(f\"\\nTotal predictions: {len(df_results)}\")\n",
        "print(f\"Provider: {provider}, Model: {model_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS: Prompt Comparison (sorted by MAE ↑, Off-by-1 ↓)\n",
            "================================================================================\n",
            "prompt  n_total  n_schema_ok coverage json_valid_rate schema_ok_rate exact_accuracy off_by_1_accuracy   mae\n",
            "    V2      200          200   100.0%          100.0%         100.0%          68.0%             98.0% 0.355\n",
            "    V3      200          200   100.0%          100.0%         100.0%          65.0%             98.5% 0.380\n",
            "    V1      200          200   100.0%          100.0%         100.0%          64.5%             98.0% 0.390\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics per prompt version\n",
        "\n",
        "def compute_metrics(df, prompt_name):\n",
        "    \"\"\"Compute evaluation metrics for a single prompt version.\"\"\"\n",
        "    subset = df[df[\"prompt\"] == prompt_name].copy()\n",
        "    n_total = len(subset)\n",
        "    \n",
        "    n_schema_ok = subset[\"schema_ok\"].sum()\n",
        "    n_valid_json = subset[\"is_valid_json\"].sum()\n",
        "    schema_ok_rate = n_schema_ok / n_total if n_total > 0 else 0\n",
        "    json_valid_rate = n_valid_json / n_total if n_total > 0 else 0\n",
        "    coverage = schema_ok_rate\n",
        "    \n",
        "    valid = subset[subset[\"schema_ok\"] == 1].copy()\n",
        "    n_valid = len(valid)\n",
        "    \n",
        "    if n_valid == 0:\n",
        "        return {\n",
        "            \"prompt\": prompt_name, \"n_total\": n_total, \"n_schema_ok\": n_schema_ok,\n",
        "            \"coverage\": coverage, \"json_valid_rate\": json_valid_rate,\n",
        "            \"schema_ok_rate\": schema_ok_rate, \"exact_accuracy\": 0.0,\n",
        "            \"off_by_1_accuracy\": 0.0, \"mae\": np.nan,\n",
        "        }\n",
        "    \n",
        "    exact_matches = (valid[\"stars_pred\"] == valid[\"stars_true\"]).sum()\n",
        "    exact_accuracy = exact_matches / n_valid\n",
        "    off_by_1 = (abs(valid[\"stars_pred\"] - valid[\"stars_true\"]) <= 1).sum()\n",
        "    off_by_1_accuracy = off_by_1 / n_valid\n",
        "    mae = abs(valid[\"stars_pred\"] - valid[\"stars_true\"]).mean()\n",
        "    \n",
        "    return {\n",
        "        \"prompt\": prompt_name, \"n_total\": n_total, \"n_schema_ok\": n_schema_ok,\n",
        "        \"coverage\": coverage, \"json_valid_rate\": json_valid_rate,\n",
        "        \"schema_ok_rate\": schema_ok_rate, \"exact_accuracy\": exact_accuracy,\n",
        "        \"off_by_1_accuracy\": off_by_1_accuracy, \"mae\": mae,\n",
        "    }\n",
        "\n",
        "metrics_list = [compute_metrics(df_results, p) for p in [\"V1\", \"V2\", \"V3\"]]\n",
        "df_comparison = pd.DataFrame(metrics_list)\n",
        "df_comparison = df_comparison.sort_values(by=[\"mae\", \"off_by_1_accuracy\"], ascending=[True, False]).reset_index(drop=True)\n",
        "\n",
        "# Format for display\n",
        "df_display = df_comparison.copy()\n",
        "for col in [\"coverage\", \"json_valid_rate\", \"schema_ok_rate\", \"exact_accuracy\", \"off_by_1_accuracy\"]:\n",
        "    df_display[col] = df_display[col].apply(lambda x: f\"{x:.1%}\")\n",
        "df_display[\"mae\"] = df_display[\"mae\"].apply(lambda x: f\"{x:.3f}\" if not np.isnan(x) else \"N/A\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS: Prompt Comparison (sorted by MAE ↑, Off-by-1 ↓)\")\n",
        "print(\"=\"*80)\n",
        "print(df_display.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Per-class accuracy (schema_ok rows only):\n",
            "------------------------------------------------------------\n",
            "V1: 1★:95% | 2★:50% | 3★:55% | 4★:52% | 5★:70%\n",
            "V2: 1★:60% | 2★:90% | 3★:52% | 4★:85% | 5★:52%\n",
            "V3: 1★:68% | 2★:88% | 3★:48% | 4★:72% | 5★:50%\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrices per prompt version\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for i, prompt_name in enumerate([\"V1\", \"V2\", \"V3\"]):\n",
        "    subset = df_results[(df_results[\"prompt\"] == prompt_name) & (df_results[\"schema_ok\"] == 1)]\n",
        "    \n",
        "    if len(subset) == 0:\n",
        "        axes[i].text(0.5, 0.5, \"No valid predictions\", ha=\"center\", va=\"center\")\n",
        "        axes[i].set_title(f\"Prompt {prompt_name}\")\n",
        "        continue\n",
        "    \n",
        "    y_true = subset[\"stars_true\"].values\n",
        "    y_pred = subset[\"stars_pred\"].values\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 2, 3, 4, 5])\n",
        "    \n",
        "    im = axes[i].imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "    axes[i].set_title(f\"Prompt {prompt_name}\\n(n={len(subset)})\")\n",
        "    axes[i].set_xlabel(\"Predicted\")\n",
        "    axes[i].set_ylabel(\"True\")\n",
        "    axes[i].set_xticks(range(5))\n",
        "    axes[i].set_yticks(range(5))\n",
        "    axes[i].set_xticklabels([1, 2, 3, 4, 5])\n",
        "    axes[i].set_yticklabels([1, 2, 3, 4, 5])\n",
        "    \n",
        "    thresh = cm.max() / 2.0\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            axes[i].text(col, row, format(cm[row, col], \"d\"),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[row, col] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Confusion Matrices by Prompt Version\", y=1.02, fontsize=14)\n",
        "\n",
        "# Save figure for later use\n",
        "confusion_fig = fig\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy breakdown\n",
        "print(\"\\nPer-class accuracy (schema_ok rows only):\")\n",
        "print(\"-\" * 60)\n",
        "per_class_data = []\n",
        "for prompt_name in [\"V1\", \"V2\", \"V3\"]:\n",
        "    subset = df_results[(df_results[\"prompt\"] == prompt_name) & (df_results[\"schema_ok\"] == 1)]\n",
        "    if len(subset) == 0:\n",
        "        continue\n",
        "    row_data = {\"prompt\": prompt_name}\n",
        "    for star in [1, 2, 3, 4, 5]:\n",
        "        star_subset = subset[subset[\"stars_true\"] == star]\n",
        "        if len(star_subset) > 0:\n",
        "            acc = (star_subset[\"stars_pred\"] == star).mean()\n",
        "            row_data[f\"{star}★\"] = f\"{acc:.0%}\"\n",
        "        else:\n",
        "            row_data[f\"{star}★\"] = \"N/A\"\n",
        "    per_class_data.append(row_data)\n",
        "    print(f\"{prompt_name}: \" + \" | \".join([f\"{star}★:{row_data[f'{star}★']}\" for star in [1,2,3,4,5]]))\n",
        "\n",
        "df_per_class = pd.DataFrame(per_class_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ARTIFACTS SAVED\n",
            "============================================================\n",
            "  ✓ outputs/prompt_comparison.csv\n",
            "  ✓ outputs/per_class_accuracy.csv\n",
            "  ✓ outputs/confusion_matrices.png\n",
            "  ✓ outputs/full_results.csv\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Save outputs to disk\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create outputs directory\n",
        "output_dir = Path(\"outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "saved_files = []\n",
        "\n",
        "# 1) Save prompt comparison CSV\n",
        "comparison_path = output_dir / \"prompt_comparison.csv\"\n",
        "df_comparison.to_csv(comparison_path, index=False)\n",
        "saved_files.append(str(comparison_path))\n",
        "\n",
        "# 2) Save per-class accuracy CSV\n",
        "per_class_path = output_dir / \"per_class_accuracy.csv\"\n",
        "df_per_class.to_csv(per_class_path, index=False)\n",
        "saved_files.append(str(per_class_path))\n",
        "\n",
        "# 3) Save confusion matrices PNG\n",
        "confusion_path = output_dir / \"confusion_matrices.png\"\n",
        "confusion_fig.savefig(confusion_path, dpi=150, bbox_inches=\"tight\")\n",
        "saved_files.append(str(confusion_path))\n",
        "\n",
        "# 4) Save full results CSV (for deeper analysis)\n",
        "full_results_path = output_dir / \"full_results.csv\"\n",
        "df_results.to_csv(full_results_path, index=False)\n",
        "saved_files.append(str(full_results_path))\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ARTIFACTS SAVED\")\n",
        "print(\"=\"*60)\n",
        "for f in saved_files:\n",
        "    print(f\"  ✓ {f}\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Running V1 on 200 rows...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V1:  34%|███▍      | 68/200 [01:19<02:35,  1.18s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m         true_stars = row[\u001b[33m\"\u001b[39m\u001b[33mstars\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     31\u001b[39m         text_len = row[\u001b[33m\"\u001b[39m\u001b[33mtext_len\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         result = \u001b[43mpredict_one_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m         all_results.append({\n\u001b[32m     41\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt_name,\n\u001b[32m     42\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstars_true\u001b[39m\u001b[33m\"\u001b[39m: true_stars,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: result.get(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     48\u001b[39m         })\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Create results dataframe\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 344\u001b[39m, in \u001b[36mpredict_one_llm\u001b[39m\u001b[34m(provider, model_name, prompt_template, review_text)\u001b[39m\n\u001b[32m    341\u001b[39m         raw = call_gemini(model_name=model_name, prompt=filled_prompt)\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    343\u001b[39m         \u001b[38;5;66;03m# This assumes you have call_openrouter defined; if not, this branch will error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m         raw = \u001b[43mcall_openrouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilled_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[32m    346\u001b[39m     error_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m call failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcall_openrouter\u001b[39m\u001b[34m(model_name, prompt, temperature, max_tokens, max_retries)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m     37\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpenRouter HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.text[:\u001b[32m500\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:937\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    922\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    939\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:1077\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1074\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1079\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:1005\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1006\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1315\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1312\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1313\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1314\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Full Evaluation: PROMPT_V1, PROMPT_V2, PROMPT_V3 on all 200 rows (OpenRouter)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "provider = \"openrouter\"\n",
        "model_name = \"qwen/qwen-2.5-7b-instruct\"\n",
        "\n",
        "# Ensure prerequisites\n",
        "if \"df_sample\" not in globals():\n",
        "    raise RuntimeError(\"df_sample is not defined. Run the data loading cell first.\")\n",
        "if \"PROMPT_V1\" not in globals() or \"PROMPT_V2\" not in globals() or \"PROMPT_V3\" not in globals():\n",
        "    raise RuntimeError(\"PROMPT_V1/V2/V3 not all defined. Run their definition cells first.\")\n",
        "\n",
        "prompts = {\n",
        "    \"V1\": PROMPT_V1,\n",
        "    \"V2\": PROMPT_V2,\n",
        "    \"V3\": PROMPT_V3,\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for prompt_name, prompt_template in prompts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running {prompt_name} on {len(df_sample)} rows...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=prompt_name):\n",
        "        review_text = row[\"text\"]\n",
        "        true_stars = row[\"stars\"]\n",
        "        text_len = row[\"text_len\"]\n",
        "        \n",
        "        result = predict_one_llm(\n",
        "            provider=provider,\n",
        "            model_name=model_name,\n",
        "            prompt_template=prompt_template,\n",
        "            review_text=review_text,\n",
        "        )\n",
        "        \n",
        "        all_results.append({\n",
        "            \"prompt\": prompt_name,\n",
        "            \"stars_true\": true_stars,\n",
        "            \"stars_pred\": result.get(\"predicted_stars\"),\n",
        "            \"schema_ok\": result.get(\"schema_ok\"),\n",
        "            \"is_valid_json\": result.get(\"is_valid_json\"),\n",
        "            \"text_len\": text_len,\n",
        "            \"error\": result.get(\"error\"),\n",
        "        })\n",
        "\n",
        "# Create results dataframe\n",
        "df_results = pd.DataFrame(all_results)\n",
        "print(f\"\\nTotal predictions: {len(df_results)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute metrics per prompt version\n",
        "\n",
        "def compute_metrics(df, prompt_name):\n",
        "    \"\"\"Compute evaluation metrics for a single prompt version.\"\"\"\n",
        "    subset = df[df[\"prompt\"] == prompt_name].copy()\n",
        "    n_total = len(subset)\n",
        "    \n",
        "    # Schema OK / JSON validity\n",
        "    n_schema_ok = subset[\"schema_ok\"].sum()\n",
        "    n_valid_json = subset[\"is_valid_json\"].sum()\n",
        "    schema_ok_rate = n_schema_ok / n_total if n_total > 0 else 0\n",
        "    json_valid_rate = n_valid_json / n_total if n_total > 0 else 0\n",
        "    coverage = schema_ok_rate  # same as schema_ok_rate for our purposes\n",
        "    \n",
        "    # Filter to schema_ok rows only for accuracy metrics\n",
        "    valid = subset[subset[\"schema_ok\"] == 1].copy()\n",
        "    n_valid = len(valid)\n",
        "    \n",
        "    if n_valid == 0:\n",
        "        return {\n",
        "            \"prompt\": prompt_name,\n",
        "            \"n_total\": n_total,\n",
        "            \"n_schema_ok\": n_schema_ok,\n",
        "            \"coverage\": coverage,\n",
        "            \"json_valid_rate\": json_valid_rate,\n",
        "            \"schema_ok_rate\": schema_ok_rate,\n",
        "            \"exact_accuracy\": 0.0,\n",
        "            \"off_by_1_accuracy\": 0.0,\n",
        "            \"mae\": np.nan,\n",
        "        }\n",
        "    \n",
        "    # Exact match accuracy\n",
        "    exact_matches = (valid[\"stars_pred\"] == valid[\"stars_true\"]).sum()\n",
        "    exact_accuracy = exact_matches / n_valid\n",
        "    \n",
        "    # Off-by-1 accuracy\n",
        "    off_by_1 = (abs(valid[\"stars_pred\"] - valid[\"stars_true\"]) <= 1).sum()\n",
        "    off_by_1_accuracy = off_by_1 / n_valid\n",
        "    \n",
        "    # MAE\n",
        "    mae = abs(valid[\"stars_pred\"] - valid[\"stars_true\"]).mean()\n",
        "    \n",
        "    return {\n",
        "        \"prompt\": prompt_name,\n",
        "        \"n_total\": n_total,\n",
        "        \"n_schema_ok\": n_schema_ok,\n",
        "        \"coverage\": coverage,\n",
        "        \"json_valid_rate\": json_valid_rate,\n",
        "        \"schema_ok_rate\": schema_ok_rate,\n",
        "        \"exact_accuracy\": exact_accuracy,\n",
        "        \"off_by_1_accuracy\": off_by_1_accuracy,\n",
        "        \"mae\": mae,\n",
        "    }\n",
        "\n",
        "# Compute metrics for each prompt\n",
        "metrics_list = []\n",
        "for prompt_name in [\"V1\", \"V2\", \"V3\"]:\n",
        "    metrics = compute_metrics(df_results, prompt_name)\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "# Create comparison dataframe\n",
        "df_comparison = pd.DataFrame(metrics_list)\n",
        "\n",
        "# Sort by MAE ascending, then off-by-1 accuracy descending\n",
        "df_comparison = df_comparison.sort_values(\n",
        "    by=[\"mae\", \"off_by_1_accuracy\"],\n",
        "    ascending=[True, False]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Format for display\n",
        "df_display = df_comparison.copy()\n",
        "df_display[\"coverage\"] = df_display[\"coverage\"].apply(lambda x: f\"{x:.1%}\")\n",
        "df_display[\"json_valid_rate\"] = df_display[\"json_valid_rate\"].apply(lambda x: f\"{x:.1%}\")\n",
        "df_display[\"schema_ok_rate\"] = df_display[\"schema_ok_rate\"].apply(lambda x: f\"{x:.1%}\")\n",
        "df_display[\"exact_accuracy\"] = df_display[\"exact_accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
        "df_display[\"off_by_1_accuracy\"] = df_display[\"off_by_1_accuracy\"].apply(lambda x: f\"{x:.1%}\")\n",
        "df_display[\"mae\"] = df_display[\"mae\"].apply(lambda x: f\"{x:.3f}\" if not np.isnan(x) else \"N/A\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS: Prompt Comparison (sorted by MAE ↑, Off-by-1 ↓)\")\n",
        "print(\"=\"*80)\n",
        "print(df_display.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices per prompt version\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for i, prompt_name in enumerate([\"V1\", \"V2\", \"V3\"]):\n",
        "    subset = df_results[(df_results[\"prompt\"] == prompt_name) & (df_results[\"schema_ok\"] == 1)]\n",
        "    \n",
        "    if len(subset) == 0:\n",
        "        axes[i].text(0.5, 0.5, \"No valid predictions\", ha=\"center\", va=\"center\")\n",
        "        axes[i].set_title(f\"Prompt {prompt_name}\")\n",
        "        continue\n",
        "    \n",
        "    y_true = subset[\"stars_true\"].values\n",
        "    y_pred = subset[\"stars_pred\"].values\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 2, 3, 4, 5])\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    im = axes[i].imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "    axes[i].set_title(f\"Prompt {prompt_name}\\n(n={len(subset)})\")\n",
        "    axes[i].set_xlabel(\"Predicted\")\n",
        "    axes[i].set_ylabel(\"True\")\n",
        "    axes[i].set_xticks(range(5))\n",
        "    axes[i].set_yticks(range(5))\n",
        "    axes[i].set_xticklabels([1, 2, 3, 4, 5])\n",
        "    axes[i].set_yticklabels([1, 2, 3, 4, 5])\n",
        "    \n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.0\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            axes[i].text(col, row, format(cm[row, col], \"d\"),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[row, col] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Confusion Matrices by Prompt Version\", y=1.02, fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy breakdown\n",
        "print(\"\\nPer-class accuracy (schema_ok rows only):\")\n",
        "print(\"-\" * 60)\n",
        "for prompt_name in [\"V1\", \"V2\", \"V3\"]:\n",
        "    subset = df_results[(df_results[\"prompt\"] == prompt_name) & (df_results[\"schema_ok\"] == 1)]\n",
        "    if len(subset) == 0:\n",
        "        print(f\"{prompt_name}: No valid predictions\")\n",
        "        continue\n",
        "    \n",
        "    class_acc = []\n",
        "    for star in [1, 2, 3, 4, 5]:\n",
        "        star_subset = subset[subset[\"stars_true\"] == star]\n",
        "        if len(star_subset) > 0:\n",
        "            acc = (star_subset[\"stars_pred\"] == star).mean()\n",
        "            class_acc.append(f\"{star}★:{acc:.0%}\")\n",
        "        else:\n",
        "            class_acc.append(f\"{star}★:N/A\")\n",
        "    \n",
        "    print(f\"{prompt_name}: {' | '.join(class_acc)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Prompt v3 (few-shot + edge cases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenRouter smoke test using PROMPT_V2\n",
        "\n",
        "provider = \"openrouter\"\n",
        "openrouter_model = \"qwen/qwen-2.5-7b-instruct\"  # can be swapped for another stable instruct model\n",
        "\n",
        "example_review = \"Amazing service and great food, will come back!\"\n",
        "\n",
        "if \"PROMPT_V2\" not in globals():\n",
        "    raise RuntimeError(\"PROMPT_V2 is not defined. Run the Prompt V2 cell first.\")\n",
        "\n",
        "result_openrouter = predict_one_llm(\n",
        "    provider=provider,\n",
        "    model_name=openrouter_model,\n",
        "    prompt_template=PROMPT_V2,\n",
        "    review_text=example_review,\n",
        ")\n",
        "\n",
        "print(result_openrouter)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
